{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 component\n",
      "[[  1.415]\n",
      " [  4.243]\n",
      " [ 14.142]\n",
      " [-11.313]\n",
      " [ -8.487]\n",
      " [  0.   ]]\n",
      "2 components\n",
      "[[  1.415  -0.119]\n",
      " [  4.243  -0.008]\n",
      " [ 14.142   0.055]\n",
      " [-11.313  -0.104]\n",
      " [ -8.487   0.207]\n",
      " [  0.     -0.03 ]]\n"
     ]
    }
   ],
   "source": [
    "# ok. You have an idea of what is PCA - finding dimensions, explaining the data the best way\n",
    "# how to run PCA?\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# print 3 symbols in fixed notation\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "\n",
    "X = np.matrix([\n",
    "    [-1, 0.1, 1],\n",
    "    [-3, 0.01, 3],\n",
    "    [-10, 0.02, 10],\n",
    "    [8, -0.01, -8],\n",
    "    [6, -0.3, -6],\n",
    "    [0, -0, 0],\n",
    "])\n",
    "\n",
    "pca = PCA(n_components=1)\n",
    "X_reduced = pca.fit_transform(X)\n",
    "X_reduced2 = PCA(n_components=2).fit_transform(X)\n",
    "\n",
    "# NB these are just distances from 0!\n",
    "print(\"1 component\")\n",
    "print(X_reduced)\n",
    "\n",
    "# NB second dimension captures remaining noise\n",
    "# It is much smaller in absolute values.\n",
    "print(\"2 components\")\n",
    "print(X_reduced2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformation matrix 1D:\n",
      "[[-0.707]\n",
      " [ 0.007]\n",
      " [ 0.707]]\n",
      "Transformed matrix 1D:\n",
      "[[  1.415]\n",
      " [  4.243]\n",
      " [ 14.142]\n",
      " [-11.313]\n",
      " [ -8.487]\n",
      " [  0.   ]]\n",
      "Transformation matrix 2D:\n",
      "[[-0.707 -0.005]\n",
      " [ 0.007 -1.   ]\n",
      " [ 0.707  0.005]]\n",
      "Transformed matrix 2D:\n",
      "[[  1.415  -0.119]\n",
      " [  4.243  -0.008]\n",
      " [ 14.142   0.055]\n",
      " [-11.313  -0.104]\n",
      " [ -8.487   0.207]\n",
      " [  0.     -0.03 ]]\n",
      "Apply trasformation to NEW data:\n",
      "[[70.716]]\n"
     ]
    }
   ],
   "source": [
    "# how to obtain transformation matrix T from 3D to 1D or 2D?\n",
    "# Obviuosly, solve an equation!\n",
    "#     (X - μ) * T = X_reduced\n",
    "# where (X - μ) is just a \"centered\" matrix X.\n",
    "# \n",
    "# This is a basic matrix equation. If (X - μ) is square, you can just\n",
    "# write T = (X - μ)^-1 * X_reduced.\n",
    "# For non-square matrix you can go with approximation of T.\n",
    "# Best well-known method is \"least square error\" method\n",
    "\n",
    "μ = np.mean(X, 0)\n",
    "X_ = X - μ\n",
    "# use LSE method\n",
    "T1 = np.linalg.pinv(X_.T * X_) * X_.T * X_reduced\n",
    "T2 = np.linalg.pinv(X_.T * X_) * X_.T * X_reduced2\n",
    "\n",
    "# Computed values are exactly the same values as PCA provided.\n",
    "# That means we solved the problem correctly.\n",
    "print(\"Transformation matrix 1D:\")\n",
    "print(T1)\n",
    "print(\"Transformed matrix X 1D:\")\n",
    "print(((X - μ) * T1))\n",
    "\n",
    "print(\"Transformation matrix 2D:\")\n",
    "print(T2)\n",
    "print(\"Transformed matrix X 2D:\")\n",
    "print(((X - μ) * T2))\n",
    "\n",
    "# But we can also project other points of bigger dimensions into smaller dimensions.\n",
    "print(\"Apply trasformation to NEW  unseen data:\")\n",
    "print(np.matrix([[-50, 1, 50]]) * T1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== U ======================\n",
      "[[-0.069 -0.445  0.874  0.134  0.123 -0.012]\n",
      " [-0.207 -0.031 -0.026  0.636 -0.732  0.123]\n",
      " [-0.69   0.206 -0.09   0.416  0.547 -0.02 ]\n",
      " [ 0.552 -0.389 -0.293  0.572  0.357 -0.066]\n",
      " [ 0.414  0.771  0.373  0.271  0.091  0.113]\n",
      " [-0.    -0.112 -0.051 -0.063  0.118  0.983]]\n",
      "==================== Σ ======================\n",
      "[[20.494  0.     0.   ]\n",
      " [ 0.     0.268  0.   ]\n",
      " [ 0.     0.     0.   ]\n",
      " [ 0.     0.     0.   ]\n",
      " [ 0.     0.     0.   ]\n",
      " [ 0.     0.     0.   ]]\n",
      "==================== V ======================\n",
      "[[ 0.707 -0.007 -0.707]\n",
      " [-0.005 -1.     0.005]\n",
      " [ 0.707 -0.     0.707]]\n",
      "Look, we got the same matrix 1D (UΣ = PCA(X))!\n",
      "[[ -1.415]\n",
      " [ -4.243]\n",
      " [-14.142]\n",
      " [ 11.313]\n",
      " [  8.487]\n",
      " [ -0.   ]]\n",
      "Look, we got the same matrix 2D (UΣ = PCA(X))!\n",
      "[[ -1.415  -0.119]\n",
      " [ -4.243  -0.008]\n",
      " [-14.142   0.055]\n",
      " [ 11.313  -0.104]\n",
      " [  8.487   0.207]\n",
      " [ -0.     -0.03 ]]\n",
      "X approximation with SVD. 1D\n",
      "[[ -1.001  -0.019   1.001]\n",
      " [ -3.      0.002   3.   ]\n",
      " [-10.      0.075  10.   ]\n",
      " [  7.999  -0.114  -7.999]\n",
      " [  6.001  -0.093  -6.001]\n",
      " [ -0.     -0.03    0.   ]]\n",
      "X approximation with SVD. 1D\n",
      "[[ -1.     0.1    1.  ]\n",
      " [ -3.     0.01   3.  ]\n",
      " [-10.     0.02  10.  ]\n",
      " [  8.    -0.01  -8.  ]\n",
      " [  6.    -0.3   -6.  ]\n",
      " [  0.    -0.     0.  ]]\n",
      "[[ 0.707 -0.007 -0.707]\n",
      " [-0.005 -1.     0.005]\n",
      " [ 0.707 -0.     0.707]]\n"
     ]
    }
   ],
   "source": [
    "# what about SVD?\n",
    "# SVD breaks a matrix into 3. It allows us to manipulate approximation of \n",
    "# initial matrix by removing rows from matrices of decomposition\n",
    "\n",
    "from numpy.linalg import svd\n",
    "\n",
    "# X - μ = exactly equals = U * Σ * V_T\n",
    "# in or example X is 6x3. Thus, U is 6x6, Σ is 6x3 and V_T is 3x3.\n",
    "\n",
    "U, s, V_T = svd(X - μ)\n",
    "print(\"==================== U ======================\")\n",
    "print(U)\n",
    "print(\"==================== Σ ======================\")\n",
    "# place values of \"s\" into diagonal of Σ\n",
    "Σ = np.zeros((U.shape[1], V_T.shape[0]), dtype=float)\n",
    "Σ[:V_T.shape[0], :V_T.shape[0]] = np.diag(s)\n",
    "print(Σ)\n",
    "print(\"==================== V ======================\")\n",
    "print(V_T)\n",
    "\n",
    "# ok. Let  X - μ = (UΣ)*V_T\n",
    "# where UΣ is 6x3 and V_T is 3x3\n",
    "# to reduce dimensions we remove middle numbers:\n",
    "# let UΣ is 6x1 and V_T is 1x3\n",
    "\n",
    "UΣ = U * Σ\n",
    "print(\"Look, we got the same matrix 1D (UΣ = PCA(X))!\")\n",
    "print(UΣ[:,:1])\n",
    "print(\"Look, we got the same matrix 2D (UΣ = PCA(X))!\")\n",
    "print(UΣ[:,:2])\n",
    "\n",
    "# restore original? - no problem.\n",
    "# X ~ UΣ[-cols] * V_T[-rows] + μ\n",
    "\n",
    "print(\"X approximation with SVD. 1D\")\n",
    "print(UΣ[:,:1] * V_T[:1, :] + μ)\n",
    "print(\"X approximation with SVD. 1D\")\n",
    "print(UΣ[:,:2] * V_T[:2, :] + μ)\n",
    "\n",
    "# to sum up. Now we have matrix T to reduce dimesions and matrix V_T to \"restore\" original vector.\n",
    "# Hmm... can we use this to generate texts?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
