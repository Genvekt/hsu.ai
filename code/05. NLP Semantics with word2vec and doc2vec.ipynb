{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# small set of tokenized sentences\n",
    "from gensim.test.utils import common_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How to check if word has an embedding? False\n",
      "How to get an embedding?\n",
      " [-0.00628895  0.00347041 -0.00010543  0.00625993 -0.00471859  0.00033526\n",
      " -0.00093802  0.00499301 -0.00419492  0.00488909 -0.00209255  0.00053695\n",
      "  0.00838132 -0.0017968   0.0012555   0.00253669 -0.00153633  0.00406475\n",
      " -0.00603754 -0.0075898  -0.00241219  0.00620627  0.00740898 -0.00450114\n",
      " -0.0008082  -0.00110063 -0.00855482  0.00120563 -0.00854596  0.00916862\n",
      " -0.00230252 -0.00542464 -0.00022875 -0.00971156  0.00114132 -0.00288143\n",
      " -0.00365368 -0.00739552 -0.00882832  0.00643377  0.00023393 -0.00682967\n",
      " -0.00655699  0.00336633 -0.00771094  0.00785643 -0.00188025 -0.00239919\n",
      " -0.00738725  0.00580115]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "\n",
    "model = Word2Vec(\n",
    "    common_texts,\n",
    "    size=50,\n",
    "    window=4,\n",
    "    min_count=2,\n",
    "    workers=10\n",
    ")\n",
    "\n",
    "# use this to CONTINUE TRAINING of already trained model\n",
    "model.train([[\"foo\", \"bar\"]], total_examples=1, epochs=10)\n",
    "\n",
    "model.wv.save(\"w2v.model\")\n",
    "wv = KeyedVectors.load(\"w2v.model\", mmap='r')\n",
    "\n",
    "print(\"How to check if word has an embedding?\", \"123\" in wv)\n",
    "print(\"How to get an embedding?\\n\", wv[\"computer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['human', 'interface', 'computer'], ['survey', 'user', 'computer', 'system', 'response', 'time'], ['eps', 'user', 'interface', 'system'], ['system', 'human', 'system', 'eps'], ['user', 'response', 'time'], ['trees'], ['graph', 'trees'], ['graph', 'minors', 'trees'], ['graph', 'minors', 'survey']] \n",
      "\n",
      "[TaggedDocument(words=['human', 'interface', 'computer'], tags=[0]), TaggedDocument(words=['survey', 'user', 'computer', 'system', 'response', 'time'], tags=[1]), TaggedDocument(words=['eps', 'user', 'interface', 'system'], tags=[2]), TaggedDocument(words=['system', 'human', 'system', 'eps'], tags=[3]), TaggedDocument(words=['user', 'response', 'time'], tags=[4]), TaggedDocument(words=['trees'], tags=[5]), TaggedDocument(words=['graph', 'trees'], tags=[6]), TaggedDocument(words=['graph', 'minors', 'trees'], tags=[7]), TaggedDocument(words=['graph', 'minors', 'survey'], tags=[8])] \n",
      "\n",
      "[ 0.04635187 -0.03873557 -0.00197775  0.07596938 -0.05825863]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "# just a test set of tokenized sentences\n",
    "print(common_texts, \"\\n\")\n",
    "documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(common_texts)]\n",
    "print(documents, \"\\n\")\n",
    "# train a model\n",
    "model = Doc2Vec(\n",
    "    documents,     # collection of texts\n",
    "    vector_size=5, # output vector size\n",
    "    window=2,      # maximum distance between the target word and its neighboring word\n",
    "    min_count=1,   # minimal number of \n",
    "    workers=4      # in parallel\n",
    ")\n",
    "\n",
    "# clean training data\n",
    "model.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)\n",
    "\n",
    "# save and load\n",
    "model.save(\"d2v.model\")\n",
    "model = Doc2Vec.load(\"d2v.model\")\n",
    "\n",
    "vec = model.infer_vector([\"system\", \"response\"])\n",
    "print(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "853461\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "853461"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"\"\n",
    "for fname in [str(i).zfill(8) + \".txt\" for i in (range(1, 475))]:\n",
    "    text += open(\"C:/Temp/MenOfGoodWill/\" + fname).read() + \"\\n\"\n",
    "print(len(text))\n",
    "open(\"C:/Temp/MenOfGoodWill/MenOfGoodWill.txt\", 'w').write(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\xenos\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "Sentences with English punkt: 36047\n",
      "Sentences with default method: 36047\n",
      "['the', 'town', 'bombard']\n",
      "216930\n",
      "OrderedDict([('Show Number', '4680'), ('Air Date', '2004-12-31'), ('Round', 'Jeopardy!'), ('Category', 'HISTORY'), ('Value', '$200'), ('Question', \"For the last 8 years of his life, Galileo was under house arrest for espousing this man's theory\"), ('Answer', 'Copernicus')])\n",
      "For the last 8 years of his life, Galileo was under house arrest for espousing this man's theory HISTORY\n",
      "This devoted mom has been called the most famous Miss America of all time MR. OR MS. WILLIAMS\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import nltk.data\n",
    "#if not downloaded yet\n",
    "nltk.download('punkt')\n",
    "from nltk import tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "# 1. train a model on a big text, e.g. War And Peace\n",
    "\n",
    "textfile = \"C:/Users/xenos/Desktop/Datasets/war and peace.txt\"\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "train = []\n",
    "with open(textfile) as f:\n",
    "    text = f.read().lower()\n",
    "    tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    sentences = tokenizer.tokenize(text)\n",
    "    print(\"Sentences with English punkt:\", len(sentences))\n",
    "    # or \n",
    "    sentences = tokenize.sent_tokenize(text)\n",
    "    print(\"Sentences with default method:\", len(sentences))\n",
    "\n",
    "    for sentence in sentences:\n",
    "        if not sentence:\n",
    "            continue\n",
    "        words = [stemmer.stem(token) for token \n",
    "                    in tokenize.word_tokenize(sentence) \n",
    "                    if token not in (',', '.', ':', '-', ';', '?', '!', '\"')\n",
    "                ]\n",
    "        train.append(words)\n",
    "\n",
    "# test\n",
    "print(train[222])\n",
    "\n",
    "##############################\n",
    "# TODO: write your code here #\n",
    "##############################\n",
    "\n",
    "# 2. embed texts of Jeopardy questions\n",
    "filename = \"C:/Users/xenos/Desktop/Datasets/JEOPARDY_CSV.csv\"\n",
    "titles = [\"Show Number\", \"Air Date\", \"Round\", \"Category\", \"Value\", \"Question\", \"Answer\"]\n",
    "s = []\n",
    "with open(filename, \"rt\", encoding=\"utf-8\") as f:\n",
    "    reader = csv.DictReader(f, quotechar='\"')\n",
    "    for line in reader:\n",
    "        s.append(line)\n",
    "\n",
    "# test\n",
    "print(len(s))\n",
    "print(s[0])\n",
    "print(s[0][titles[5]], s[0][titles[3]])\n",
    "print(s[1000][titles[5]], s[1000][titles[3]])\n",
    "\n",
    "##############################\n",
    "# TODO: write your code here #\n",
    "##############################\n",
    "\n",
    "# 3. For a given new question propose new topic using K-nearest neghbours approach\n",
    "\n",
    "##############################\n",
    "# TODO: write your code here #\n",
    "##############################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
