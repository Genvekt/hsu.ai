{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences with English punkt: 1923\n",
      "Sentences with default method: 1923\n",
      "he rowed slowly and steadily toward where the bird was circling.\n",
      "['he', 'rowed', 'slowly', 'and', 'steadily', 'toward', 'where', 'the', 'bird', 'was', 'circling']\n",
      "['he', 'row', 'slowli', 'and', 'steadili', 'toward', 'where', 'the', 'bird', 'was', 'circl']\n"
     ]
    }
   ],
   "source": [
    "# if not downloaded yet\n",
    "# nltk.download('punkt')\n",
    "import nltk\n",
    "from nltk import tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "# try simple engish\n",
    "textfile = \"datasets/nlp/the old man and the sea.txt\"\n",
    "# or more complex\n",
    "# textfile = \"datasets/nlp/MenOfGoodWill.txt\"\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "sentences = []\n",
    "words = []\n",
    "lexemes = []\n",
    "with open(textfile) as f:\n",
    "    text = f.read().lower()\n",
    "    \n",
    "    # lets split text for sentences first\n",
    "    \n",
    "    # these 2 parts are the same. Either complex one:\n",
    "    tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    sentences = tokenizer.tokenize(text)\n",
    "    print(\"Sentences with English punkt:\", len(sentences))\n",
    "    # or \"from the box\"\n",
    "    sentences = tokenize.sent_tokenize(text)\n",
    "    print(\"Sentences with default method:\", len(sentences))\n",
    "\n",
    "    \n",
    "    # let's explode sentences to lexemes\n",
    "    for sentence in sentences:\n",
    "        if not sentence:\n",
    "            continue\n",
    "        s_words = [word for word\n",
    "                    in tokenize.word_tokenize(sentence)\n",
    "                    if word not in (',', '.', ':', '-', ';', '?', '!', '\"', \"``\", \"`\", \"''\")\n",
    "                ]\n",
    "        s_lexemes = [stemmer.stem(word) for word in s_words]\n",
    "        words.append(s_words)\n",
    "        lexemes.append(s_lexemes)\n",
    "\n",
    "# test\n",
    "print(sentences[400])\n",
    "print(words[400])\n",
    "print(lexemes[400])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "lexicon1 = set(itertools.chain(*words))\n",
    "print(\"Words lexicon:\", len(lexicon1))\n",
    "\n",
    "lexicon2 = set(itertools.chain(*lexemes))\n",
    "print(\"Words lexicon:\", len(lexicon2))\n",
    "\n",
    "# will the numbers change if you change text file?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fish {'fished', 'fish', 'fishes', 'fishing'}\n",
      "form {'formed', 'form', 'forms'}\n",
      "see {'see', 'seeing', 'sees'}\n",
      "come {'come', 'coming', 'comes'}\n",
      "carri {'carrying', 'carried', 'carry'}\n",
      "coil {'coil', 'coiled', 'coils'}\n",
      "sail {'sailed', 'sailing', 'sail', 'sails'}\n",
      "patch {'patch', 'patches', 'patched'}\n",
      "look {'looked', 'looks', 'look', 'looking'}\n",
      "back {'back', 'backing', 'backs', 'backed'}\n",
      "skin {'skin', 'skinning', 'skinned'}\n",
      "bring {'bring', 'bringing', 'brings'}\n",
      "love {'loving', 'lovely', 'love', 'loved'}\n",
      "stay {'stay', 'stays', 'stayed'}\n",
      "rememb {'remembers', 'remembered', 'remember'}\n",
      "know {'knows', 'know', 'knowing'}\n",
      "leav {'leaves', 'leaving', 'leave'}\n",
      "show {'show', 'showed', 'showing', 'shows'}\n",
      "drift {'drifted', 'drifting', 'drift'}\n",
      "plank {'planking', 'planks', 'plank'}\n",
      "end {'ended', 'end', 'ends'}\n",
      "wait {'wait', 'waiting', 'waited'}\n",
      "cut {'cutting', 'cuts', 'cut'}\n",
      "salt {'salted', 'salt', 'salting'}\n",
      "smell {'smell', 'smelling', 'smelled'}\n",
      "drop {'drops', 'dropping', 'dropped', 'drop'}\n",
      "think {'think', 'thinking', 'thinks'}\n",
      "get {'getting', 'get', 'gets'}\n",
      "play {'played', 'play', 'playing'}\n",
      "row {'rowing', 'row', 'rows', 'rowed'}\n",
      "kill {'killing', 'killed', 'kill', 'kills'}\n",
      "club {'clubbing', 'club', 'clubbed'}\n",
      "feel {'feel', 'feelings', 'feeling'}\n",
      "shiver {'shivered', 'shivering', 'shiver'}\n",
      "chop {'chop', 'chopped', 'chopping'}\n",
      "confid {'confident', 'confidence', 'confided'}\n",
      "bait {'baits', 'bait', 'baited'}\n",
      "rise {'rises', 'rising', 'rise'}\n",
      "thank {'thanks', 'thank', 'thanked'}\n",
      "ask {'asked', 'asking', 'ask'}\n",
      "shift {'shifts', 'shifted', 'shifting', 'shift'}\n",
      "want {'wanted', 'wants', 'want'}\n",
      "light {'lightness', 'lights', 'lighted', 'lightly', 'light'}\n",
      "tri {'tried', 'trying', 'try'}\n",
      "work {'work', 'worked', 'working'}\n",
      "hook {'hooked', 'hooks', 'hook'}\n",
      "strang {'strangely', 'strange', 'strangeness'}\n",
      "shoulder {'shoulder', 'shouldered', 'shoulders'}\n",
      "use {'using', 'use', 'used'}\n",
      "thought {'thoughtful', 'thought', 'thoughts'}\n",
      "open {'opened', 'open', 'opening'}\n",
      "place {'placed', 'place', 'places'}\n",
      "clean {'clean', 'cleaned', 'cleanly'}\n",
      "make {'making', 'make', 'makes'}\n",
      "pound {'pounds', 'pound', 'pounded'}\n",
      "keep {'keep', 'keeping', 'keeps'}\n",
      "lose {'losing', 'lose', 'loses'}\n",
      "care {'careful', 'carefully', 'care'}\n",
      "head {'headed', 'heads', 'head'}\n",
      "close {'close', 'closed', 'closing'}\n",
      "arm {'armed', 'arms', 'arm'}\n",
      "start {'start', 'started', 'starting'}\n",
      "live {'live', 'lived', 'lives'}\n",
      "set {'sets', 'setting', 'set'}\n",
      "need {'needs', 'needed', 'need'}\n",
      "mean {'meaning', 'mean', 'means'}\n",
      "drive {'driving', 'drive', 'drives'}\n",
      "hit {'hits', 'hitting', 'hit'}\n",
      "talk {'talking', 'talked', 'talk'}\n",
      "drink {'drinking', 'drinks', 'drink'}\n",
      "prove {'proving', 'prove', 'proved'}\n",
      "sleep {'sleeping', 'sleep', 'sleeps'}\n",
      "hard {'hardly', 'hard', 'hardness'}\n",
      "roll {'rolls', 'rolling', 'rolled'}\n",
      "dream {'dreaming', 'dreams', 'dream', 'dreamed'}\n",
      "fight {'fight', 'fights', 'fighting'}\n",
      "clear {'cleared', 'clearing', 'clearly', 'clear'}\n",
      "die {'dying', 'died', 'dies', 'die'}\n",
      "turn {'turn', 'turns', 'turned'}\n",
      "pull {'pulls', 'pull', 'pulled', 'pulling'}\n",
      "move {'moving', 'move', 'moved'}\n",
      "lash {'lashing', 'lash', 'lashings', 'lashed'}\n",
      "dip {'dipping', 'dipped', 'dip'}\n",
      "friend {'friendly', 'friend', 'friends'}\n",
      "swallow {'swallowing', 'swallow', 'swallows'}\n",
      "beauti {'beautiful', 'beauty', 'beautifully'}\n",
      "float {'floats', 'floating', 'floated'}\n",
      "solid {'solidly', 'solid', 'solidity'}\n",
      "tast {'tasting', 'tasted', 'taste'}\n",
      "loop {'looped', 'loop', 'loops'}\n",
      "watch {'watched', 'watch', 'watching'}\n",
      "circl {'circle', 'circles', 'circled', 'circling'}\n",
      "slant {'slanting', 'slanted', 'slant'}\n",
      "desper {'desperate', 'desperately', 'desperation'}\n",
      "rais {'raised', 'raising', 'raise'}\n",
      "cloud {'clouds', 'cloud', 'clouding'}\n",
      "poison {'poisoning', 'poison', 'poisonings'}\n",
      "rest {'rested', 'rest', 'resting'}\n",
      "step {'step', 'stepping', 'stepped'}\n",
      "hate {'hated', 'hate', 'hateful'}\n",
      "jump {'jump', 'jumping', 'jumps', 'jumped'}\n",
      "leap {'leap', 'leaped', 'leaping'}\n",
      "travel {'travelled', 'travelling', 'travel', 'travels'}\n",
      "increas {'increase', 'increased', 'increasing'}\n",
      "steer {'steered', 'steer', 'steering'}\n",
      "consid {'consider', 'considered', 'considering'}\n",
      "bump {'bumping', 'bumped', 'bumps'}\n",
      "pleas {'pleased', 'pleases', 'please'}\n",
      "slip {'slip', 'slipped', 'slipping'}\n",
      "gain {'gaining', 'gain', 'gained'}\n",
      "slow {'slowing', 'slowed', 'slow'}\n",
      "jerk {'jerk', 'jerked', 'jerking'}\n",
      "cramp {'cramps', 'cramping', 'cramped', 'cramp'}\n",
      "chang {'changed', 'changes', 'change'}\n",
      "dri {'dry', 'drying', 'dried'}\n",
      "comfort {'comfort', 'comfortable', 'comfortably'}\n",
      "prepar {'prepare', 'prepared', 'preparing', 'preparation'}\n",
      "sever {'several', 'sever', 'severed'}\n",
      "bleed {'bleeds', 'bleed', 'bleeding'}\n",
      "burn {'burn', 'burned', 'burning'}\n",
      "chew {'chewing', 'chew', 'chewed'}\n",
      "intellig {'intelligent', 'intelligently', 'intelligence'}\n",
      "pass {'passes', 'pass', 'passed'}\n",
      "gut {'gutted', 'gut', 'guts'}\n"
     ]
    }
   ],
   "source": [
    "clusters = {}\n",
    "for i in range(len(lexemes)):\n",
    "    for j in range(len(lexemes[i])):\n",
    "        # TODO: fill the dictionary or clusters\n",
    "        \n",
    "for key, value in clusters.items():\n",
    "    if len(value) > 2:\n",
    "        print(key, value)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
