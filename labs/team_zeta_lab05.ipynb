{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "def load_database(textfile):\n",
    "    sentences = []\n",
    "    words = []\n",
    "    lexemes = []\n",
    "    with open(textfile) as f:\n",
    "        text = f.read().lower()\n",
    "        sentences = tokenize.sent_tokenize(text)\n",
    "        for sentence in sentences:\n",
    "            if not sentence:\n",
    "                continue\n",
    "            s_words = [word for word\n",
    "                        in tokenize.word_tokenize(sentence)\n",
    "                        if word not in (',', '.', ':', '-', ';', '?', '!', '\"', \"``\", \"`\", \"''\")\n",
    "                    ]\n",
    "            s_lexemes = [stemmer.stem(word) for word in s_words]\n",
    "            words.append(s_words)\n",
    "            lexemes.append(s_lexemes)\n",
    "    return sentences, words, lexemes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences, words, lexemes = load_database(\"datasets/nlp/the old man and the sea.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def amount_contains_sentence(bag, sentence):\n",
    "    amount = 0\n",
    "    for word in bag:\n",
    "        if word in sentence:\n",
    "            amount += 1\n",
    "            \n",
    "    return amount\n",
    "# TODO: write the code that will find ALL sentences which contain all words of query\n",
    "def exact_match(query):\n",
    "    result = []\n",
    "    stemmed_bag_query = [stemmer.stem(word.lower()) for word in query.split()]\n",
    "    \n",
    "    for sentence in lexemes:\n",
    "        if amount_contains_sentence(stemmed_bag_query, sentence) == len(stemmed_bag_query):\n",
    "            result.append(sentence)\n",
    "    \n",
    "    return result\n",
    "\n",
    "# TODO: write the code that will find TOP sentences with THE BEST matches with query\n",
    "def ranked_match(query, top=5):\n",
    "    ranked_result = []\n",
    "    stemmed_bag_query = [stemmer.stem(word.lower()) for word in query.split()]\n",
    "    \n",
    "    for sentence in lexemes:\n",
    "        amount = amount_contains_sentence(stemmed_bag_query, sentence)\n",
    "        if amount > 0:\n",
    "            ranked_result.append((amount, sentence))\n",
    "    \n",
    "    ranked_result.sort(reverse=True)\n",
    "    return [item[1] for item in ranked_result[-top:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['it', 'was', 'the', 'yellow', 'gulf', 'weed', 'that', 'had', 'made', 'so', 'much', 'phosphoresc', 'in', 'the', 'night'], ['so', 'he', 'hook', 'a', 'patch', 'of', 'yellow', 'gulf', 'weed', 'with', 'the', 'gaff', 'as', 'they', 'pass', 'and', 'shook', 'it', 'so', 'that', 'the', 'small', 'shrimp', 'that', 'were', 'in', 'it', 'fell', 'onto', 'the', 'plank', 'of', 'the', 'skiff']]\n",
      "\n",
      "[['the', 'dark', 'water', 'of', 'the', 'true', 'gulf', 'is', 'the', 'greatest', 'healer', 'that', 'there', 'is'], ['the', 'boy', 'had', 'given', 'him', 'two', 'fresh', 'small', 'tuna', 'or', 'albacor', 'which', 'hung', 'on', 'the', 'two', 'deepest', 'line', 'like', 'plummet', 'and', 'on', 'the', 'other', 'he', 'had', 'a', 'big', 'blue', 'runner', 'and', 'a', 'yellow', 'jack', 'that', 'had', 'been', 'use', 'befor', 'but', 'they', 'were', 'in', 'good', 'condit', 'still', 'and', 'had', 'the', 'excel', 'sardin', 'to', 'give', 'them', 'scent', 'and', 'attract'], ['he', 'love', 'green', 'turtl', 'and', 'hawks-bil', 'with', 'their', 'eleg', 'and', 'speed', 'and', 'their', 'great', 'valu', 'and', 'he', 'had', 'a', 'friend', 'contempt', 'for', 'the', 'huge', 'stupid', 'loggerhead', 'yellow', 'in', 'their', 'armour-pl', 'strang', 'in', 'their', 'love-mak', 'and', 'happili', 'eat', 'the', 'portugues', 'men-of-war', 'with', 'their', 'eye', 'shut'], ['after', 'that', 'he', 'began', 'to', 'dream', 'of', 'the', 'long', 'yellow', 'beach', 'and', 'he', 'saw', 'the', 'first', 'of', 'the', 'lion', 'come', 'down', 'onto', 'it', 'in', 'the', 'earli', 'dark', 'and', 'then', 'the', 'other', 'lion', 'came', 'and', 'he', 'rest', 'his', 'chin', 'on', 'the', 'wood', 'of', 'the', 'bow', 'where', 'the', 'ship', 'lay', 'anchor', 'with', 'the', 'even', 'off-shor', 'breez', 'and', 'he', 'wait', 'to', 'see', 'if', 'there', 'would', 'be', 'more', 'lion', 'and', 'he', 'was', 'happi'], ['a', 'pot', 'of', 'yellow', 'rice', 'with', 'fish']]\n"
     ]
    }
   ],
   "source": [
    "print(exact_match(\"yellow Gulf weeded\"))\n",
    "print()\n",
    "print(ranked_match(\"yellow Gulf weeded\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
