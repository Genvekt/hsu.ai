{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences with English punkt: 1923\n",
      "Sentences with default method: 1923\n",
      "he rowed slowly and steadily toward where the bird was circling.\n",
      "['he', 'rowed', 'slowly', 'and', 'steadily', 'toward', 'where', 'the', 'bird', 'was', 'circling']\n",
      "['he', 'row', 'slowli', 'and', 'steadili', 'toward', 'where', 'the', 'bird', 'was', 'circl']\n"
     ]
    }
   ],
   "source": [
    "# if not downloaded yet\n",
    "# nltk.download('punkt')\n",
    "import nltk\n",
    "from nltk import tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "# try simple engish\n",
    "textfile = r\"C:\\Users\\Anouar\\Desktop\\the old man and the sea.txt\"\n",
    "# or more complex\n",
    "# textfile = \"datasets/nlp/MenOfGoodWill.txt\"\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "sentences = []\n",
    "words = []\n",
    "lexemes = []\n",
    "with open(textfile) as f:\n",
    "    text = f.read().lower()\n",
    "    \n",
    "    # lets split text for sentences first\n",
    "    \n",
    "    # these 2 parts are the same. Either complex one:\n",
    "    tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    sentences = tokenizer.tokenize(text)\n",
    "    print(\"Sentences with English punkt:\", len(sentences))\n",
    "    # or \"from the box\"\n",
    "    sentences = tokenize.sent_tokenize(text)\n",
    "    print(\"Sentences with default method:\", len(sentences))\n",
    "\n",
    "    \n",
    "    # let's explode sentences to lexemes\n",
    "    for sentence in sentences:\n",
    "        if not sentence:\n",
    "            continue\n",
    "        s_words = [word for word\n",
    "                    in tokenize.word_tokenize(sentence)\n",
    "                    if word not in (',', '.', ':', '-', ';', '?', '!', '\"', \"``\", \"`\", \"''\")\n",
    "                ]\n",
    "        s_lexemes = [stemmer.stem(word) for word in s_words]\n",
    "        words.append(s_words)\n",
    "        lexemes.append(s_lexemes)\n",
    "\n",
    "# test\n",
    "print(sentences[400])\n",
    "print(words[400])\n",
    "print(lexemes[400])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words lexicon: 2498\n",
      "Words lexicon: 1925\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "lexicon1 = set(itertools.chain(*words))\n",
    "print(\"Words lexicon:\", len(lexicon1))\n",
    "\n",
    "lexicon2 = set(itertools.chain(*lexemes))\n",
    "print(\"Words lexicon:\", len(lexicon2))\n",
    "\n",
    "# will the numbers change if you change text file?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fish {'fishes', 'fish', 'fished', 'fishing'}\n",
      "form {'form', 'forms', 'formed'}\n",
      "see {'see', 'seeing', 'sees'}\n",
      "come {'comes', 'come', 'coming'}\n",
      "carri {'carried', 'carrying', 'carry'}\n",
      "coil {'coils', 'coil', 'coiled'}\n",
      "sail {'sailing', 'sailed', 'sails', 'sail'}\n",
      "patch {'patches', 'patch', 'patched'}\n",
      "look {'looked', 'looks', 'look', 'looking'}\n",
      "back {'back', 'backing', 'backed', 'backs'}\n",
      "skin {'skinning', 'skinned', 'skin'}\n",
      "bring {'bringing', 'bring', 'brings'}\n",
      "love {'love', 'loving', 'loved', 'lovely'}\n",
      "stay {'stayed', 'stay', 'stays'}\n",
      "rememb {'remember', 'remembered', 'remembers'}\n",
      "know {'knows', 'know', 'knowing'}\n",
      "leav {'leave', 'leaves', 'leaving'}\n",
      "show {'showed', 'show', 'showing', 'shows'}\n",
      "drift {'drift', 'drifted', 'drifting'}\n",
      "plank {'plank', 'planking', 'planks'}\n",
      "end {'ended', 'ends', 'end'}\n",
      "wait {'waiting', 'wait', 'waited'}\n",
      "cut {'cut', 'cutting', 'cuts'}\n",
      "salt {'salting', 'salt', 'salted'}\n",
      "smell {'smelling', 'smell', 'smelled'}\n",
      "drop {'drop', 'dropping', 'dropped', 'drops'}\n",
      "think {'thinking', 'thinks', 'think'}\n",
      "get {'get', 'gets', 'getting'}\n",
      "play {'play', 'played', 'playing'}\n",
      "row {'row', 'rowed', 'rows', 'rowing'}\n",
      "kill {'kill', 'killed', 'killing', 'kills'}\n",
      "club {'clubbed', 'club', 'clubbing'}\n",
      "feel {'feel', 'feelings', 'feeling'}\n",
      "shiver {'shivered', 'shivering', 'shiver'}\n",
      "chop {'chop', 'chopped', 'chopping'}\n",
      "confid {'confided', 'confident', 'confidence'}\n",
      "bait {'bait', 'baited', 'baits'}\n",
      "rise {'rise', 'rising', 'rises'}\n",
      "thank {'thanked', 'thank', 'thanks'}\n",
      "ask {'ask', 'asked', 'asking'}\n",
      "shift {'shifting', 'shifted', 'shift', 'shifts'}\n",
      "want {'wanted', 'want', 'wants'}\n",
      "light {'lighted', 'lightness', 'lights', 'lightly', 'light'}\n",
      "tri {'trying', 'tried', 'try'}\n",
      "work {'worked', 'working', 'work'}\n",
      "hook {'hooked', 'hook', 'hooks'}\n",
      "strang {'strangely', 'strange', 'strangeness'}\n",
      "shoulder {'shoulder', 'shoulders', 'shouldered'}\n",
      "use {'use', 'used', 'using'}\n",
      "thought {'thought', 'thoughtful', 'thoughts'}\n",
      "open {'open', 'opened', 'opening'}\n",
      "place {'places', 'placed', 'place'}\n",
      "clean {'cleaned', 'clean', 'cleanly'}\n",
      "make {'makes', 'make', 'making'}\n",
      "pound {'pounded', 'pound', 'pounds'}\n",
      "keep {'keeping', 'keeps', 'keep'}\n",
      "lose {'losing', 'loses', 'lose'}\n",
      "care {'care', 'careful', 'carefully'}\n",
      "head {'heads', 'head', 'headed'}\n",
      "close {'closing', 'close', 'closed'}\n",
      "arm {'armed', 'arms', 'arm'}\n",
      "start {'starting', 'start', 'started'}\n",
      "live {'lives', 'lived', 'live'}\n",
      "set {'sets', 'setting', 'set'}\n",
      "need {'needed', 'needs', 'need'}\n",
      "mean {'mean', 'meaning', 'means'}\n",
      "drive {'drive', 'drives', 'driving'}\n",
      "hit {'hitting', 'hit', 'hits'}\n",
      "talk {'talked', 'talk', 'talking'}\n",
      "drink {'drinking', 'drinks', 'drink'}\n",
      "prove {'proving', 'proved', 'prove'}\n",
      "sleep {'sleep', 'sleeping', 'sleeps'}\n",
      "hard {'hardly', 'hardness', 'hard'}\n",
      "roll {'rolled', 'rolls', 'rolling'}\n",
      "dream {'dream', 'dreams', 'dreaming', 'dreamed'}\n",
      "fight {'fighting', 'fight', 'fights'}\n",
      "clear {'cleared', 'clear', 'clearing', 'clearly'}\n",
      "die {'dying', 'die', 'died', 'dies'}\n",
      "turn {'turns', 'turn', 'turned'}\n",
      "pull {'pulling', 'pulled', 'pull', 'pulls'}\n",
      "move {'moving', 'move', 'moved'}\n",
      "lash {'lashed', 'lashings', 'lash', 'lashing'}\n",
      "dip {'dipped', 'dipping', 'dip'}\n",
      "friend {'friends', 'friendly', 'friend'}\n",
      "swallow {'swallowing', 'swallows', 'swallow'}\n",
      "beauti {'beauty', 'beautifully', 'beautiful'}\n",
      "float {'floating', 'floated', 'floats'}\n",
      "solid {'solidity', 'solid', 'solidly'}\n",
      "tast {'taste', 'tasted', 'tasting'}\n",
      "loop {'loops', 'loop', 'looped'}\n",
      "watch {'watch', 'watching', 'watched'}\n",
      "circl {'circle', 'circled', 'circles', 'circling'}\n",
      "slant {'slanted', 'slanting', 'slant'}\n",
      "desper {'desperate', 'desperately', 'desperation'}\n",
      "rais {'raising', 'raise', 'raised'}\n",
      "cloud {'clouding', 'cloud', 'clouds'}\n",
      "poison {'poisoning', 'poisonings', 'poison'}\n",
      "rest {'rested', 'rest', 'resting'}\n",
      "step {'stepped', 'stepping', 'step'}\n",
      "hate {'hateful', 'hated', 'hate'}\n",
      "jump {'jump', 'jumps', 'jumping', 'jumped'}\n",
      "leap {'leaping', 'leap', 'leaped'}\n",
      "travel {'travel', 'travelling', 'travelled', 'travels'}\n",
      "increas {'increased', 'increasing', 'increase'}\n",
      "steer {'steering', 'steer', 'steered'}\n",
      "consid {'consider', 'considering', 'considered'}\n",
      "bump {'bumped', 'bumps', 'bumping'}\n",
      "pleas {'please', 'pleased', 'pleases'}\n",
      "slip {'slipped', 'slipping', 'slip'}\n",
      "gain {'gaining', 'gained', 'gain'}\n",
      "slow {'slowing', 'slow', 'slowed'}\n",
      "jerk {'jerk', 'jerking', 'jerked'}\n",
      "cramp {'cramps', 'cramping', 'cramp', 'cramped'}\n",
      "chang {'changed', 'change', 'changes'}\n",
      "dri {'dry', 'dried', 'drying'}\n",
      "comfort {'comfort', 'comfortable', 'comfortably'}\n",
      "prepar {'preparing', 'preparation', 'prepare', 'prepared'}\n",
      "sever {'severed', 'sever', 'several'}\n",
      "bleed {'bleeding', 'bleeds', 'bleed'}\n",
      "burn {'burned', 'burning', 'burn'}\n",
      "chew {'chew', 'chewed', 'chewing'}\n",
      "intellig {'intelligence', 'intelligent', 'intelligently'}\n",
      "pass {'passed', 'pass', 'passes'}\n",
      "gut {'gutted', 'guts', 'gut'}\n"
     ]
    }
   ],
   "source": [
    "clusters = {}\n",
    "for i in lexemes:\n",
    "    for j in i:\n",
    "        clusters[j] = set()\n",
    "        \n",
    "for i in words:\n",
    "    for j in i:\n",
    "        clusters[stemmer.stem(j)].add(j)\n",
    "        \n",
    "for key, value in clusters.items():\n",
    "    if len(value) > 2:\n",
    "        print(key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
