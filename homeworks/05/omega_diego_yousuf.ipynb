{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import itertools\n",
    "from collections import Counter\n",
    "import math\n",
    "\n",
    "def load_database(textfile):\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    sentences = []\n",
    "    words = []\n",
    "    lexemes = []\n",
    "    with open(textfile) as f:\n",
    "        text = f.read().lower()\n",
    "        sentences = tokenize.sent_tokenize(text)\n",
    "        for sentence in sentences:\n",
    "            if not sentence:\n",
    "                continue\n",
    "            s_words = [word for word\n",
    "                        in tokenize.word_tokenize(sentence)\n",
    "                        if word not in (',', '.', ':', '-', ';', '?', '!', '\"', \"``\", \"`\", \"''\")\n",
    "                    ]\n",
    "            s_lexemes = [stemmer.stem(word) for word in s_words]\n",
    "            words.append(s_words)\n",
    "            lexemes.append(s_lexemes)\n",
    "    return sentences, words, lexemes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences, words, lexemes = load_database(\"C:/Users/diego/PycharmProjects/AI/week_1/Homework/the old man and the sea.txt\")\n",
    "text_stemmed = list(itertools.chain(*lexemes))\n",
    "sentences_stemmed = []\n",
    "for i in range(len(lexemes)):\n",
    "    temp = \" \".join(lexemes[i])\n",
    "    sentences_stemmed.append(temp)\n",
    "stemmed_lexicon = Counter(text_stemmed).most_common(len(text_stemmed))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: write the code that will find ALL sentences which contain all words of query\n",
    "def exact_match(query):\n",
    "    result = []\n",
    "    query = query.lower()\n",
    "    for sentence in sentences:\n",
    "        if query in sentence:\n",
    "            result.append(sentence)\n",
    "    return result\n",
    "\n",
    "# TODO: write the code that will find TOP sentences with THE BEST matches with query\n",
    "def ranked_match(query, top=5):\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    ranked_result = []\n",
    "    intersection = []\n",
    "    intersection_stemmed = []\n",
    "    query = [word.lower() for word in query.split(\" \")]\n",
    "    query = [stemmer.stem(word) for word in query]\n",
    "    \n",
    "    for word in query:\n",
    "        for i in range(len(lexemes)):\n",
    "            for j in range(len(lexemes[i])):\n",
    "                if word == lexemes[i][j]:\n",
    "                    if sentences[i] not in intersection:\n",
    "                        intersection.append(sentences[i])\n",
    "                        intersection_stemmed.append(sentences_stemmed[i])\n",
    "    \n",
    "    f = []\n",
    "    N = len(lexemes)\n",
    "    n = len(intersection)\n",
    "    search_result = []\n",
    "                \n",
    "    for sentence in intersection_stemmed:\n",
    "        s = 0\n",
    "        number_of_words = len(sentence.split())\n",
    "        for word in query: \n",
    "            n_temp = []\n",
    "            if word in sentence:\n",
    "                n_temp.append(sentence)\n",
    "\n",
    "            f_word = sentence.count(word)\n",
    "            if len(n_temp) == 0:\n",
    "                s += 0\n",
    "            else:\n",
    "                s += ( 0.5 + 0.5 *  f_word / number_of_words) * math.log(N / len(n_temp))\n",
    "\n",
    "        search_result.append(s)   \n",
    "\n",
    "    for i in range(n):\n",
    "        ranked_result.append((intersection[i], search_result[i]))\n",
    "    \n",
    "    ranked_result.sort(key=lambda x: x[1], reverse = True)     \n",
    "    return [item[0] for item in ranked_result[:5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ranked_cos(query, top=5):\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    ranked_result = []\n",
    "    intersection = []\n",
    "    intersection_stemmed = []\n",
    "    intersection_word = []\n",
    "    query_vector = []\n",
    "    sentence_vector = []\n",
    "    Search = []\n",
    "    query = [word.lower() for word in query.split(\" \")]\n",
    "    query = [stemmer.stem(word) for word in query]\n",
    "    \n",
    "    for word in query:\n",
    "        for i in range(len(lexemes)):\n",
    "            for j in range(len(lexemes[i])):\n",
    "                if word == lexemes[i][j]:\n",
    "                    if sentences[i] not in intersection:\n",
    "                        intersection.append(sentences[i])\n",
    "                        intersection_stemmed.append(sentences_stemmed[i])\n",
    "\n",
    "    for sentence in intersection_stemmed:\n",
    "        s_words = [word for word\n",
    "                        in tokenize.word_tokenize(sentence)\n",
    "                        if word not in (',', '.', ':', '-', ';', '?', '!', '\"', \"``\", \"`\", \"''\")\n",
    "                    ]\n",
    "        \n",
    "        intersection_word.append(s_words)\n",
    "    \n",
    "    for sentence in intersection_word:\n",
    "        query_temporal = []\n",
    "        sentence_temporal = []\n",
    "        for word in sentence:\n",
    "            check = False\n",
    "            for quer in query:\n",
    "                if word == quer: \n",
    "                    check = True\n",
    "                    break\n",
    "            if not check:\n",
    "                query_temporal.append(0)\n",
    "            else:\n",
    "                query_temporal.append(1)\n",
    "                \n",
    "            for i in range(len(stemmed_lexicon)):\n",
    "                if word == stemmed_lexicon[i][0]:\n",
    "                        sentence_temporal.append(stemmed_lexicon[i][1])\n",
    "               \n",
    "        query_vector.append(query_temporal)\n",
    "        sentence_vector.append(sentence_temporal)\n",
    "    \n",
    "    AB = 0\n",
    "    A_sq = 0\n",
    "    B_sq = 0\n",
    "    temp_result = 0\n",
    "    for i in range(len(query_vector)):\n",
    "        for j in range(len(query_vector[i])):\n",
    "            AB += query_vector[i][j]*sentence_vector[i][j]\n",
    "            A_sq += query_vector[i][j]**2\n",
    "            B_sq += sentence_vector[i][j]**2\n",
    "        temp_result = AB/(A_sq**0.5*B_sq**0.5)\n",
    "        Search.append(temp_result)\n",
    "        AB = 0\n",
    "        A_sq = 0\n",
    "        B_sq = 0\n",
    "    \n",
    "    for i in range(len(Search)):\n",
    "        ranked_result.append((intersection[i], Search[i]))\n",
    "    \n",
    "    ranked_result.sort(key=lambda x: x[1], reverse = True)\n",
    "    return [item[0] for item in ranked_result[:5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['it was the yellow gulf weed that had made so much phosphorescence in the night.', 'so he hooked a patch of yellow gulf weed with the gaff as they passed and shook it so that the small shrimps that were in it fell onto the planking of the skiff.']\n",
      "['it was the yellow gulf weed that had made so much phosphorescence in the night.', 'so he hooked a patch of yellow gulf weed with the gaff as they passed and shook it so that the small shrimps that were in it fell onto the planking of the skiff.', 'there was yellow weed on the line but the old man knew that only made an added drag and he was pleased.', 'there were only the flying fish that went up from his bow sailing away to either side and the yellow patches of gulf-weed.', 'but the bird was almost out of sight now and nothing showed on the surface of the water but some patches of yellow, sun-bleached sargasso weed and the purple, formalized, iridescent, gelatinous bladder of a portuguese man-of-war floating close beside the boat.']\n",
      "['\"a pot of yellow rice with fish.', 'there was no pot of yellow rice and fish and the boy knew this too.', 'it was the yellow gulf weed that had made so much phosphorescence in the night.', 'there were only the flying fish that went up from his bow sailing away to either side and the yellow patches of gulf-weed.', 'just before it was dark, as they passed a great island of sargasso weed that heaved and swung in the light sea as though the ocean were making love with something under a yellow blanket, his small line was taken by a dolphin.']\n"
     ]
    }
   ],
   "source": [
    "print(exact_match(\"yellow Gulf weed\"))\n",
    "print(ranked_match('yellow Gulf weed'))\n",
    "print(ranked_cos(\"yellow Gulf weed\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
