{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "def load_database(textfile):\n",
    "    sentences = []\n",
    "    words = []\n",
    "    lexemes = []\n",
    "    with open(textfile) as f:\n",
    "        text = f.read().lower()\n",
    "        sentences = tokenize.sent_tokenize(text)\n",
    "        for sentence in sentences:\n",
    "            if not sentence:\n",
    "                continue\n",
    "            s_words = [word for word\n",
    "                        in tokenize.word_tokenize(sentence)\n",
    "                        if word not in (',', '.', ':', '-', ';', '?', '!', '\"', \"``\", \"`\", \"''\")\n",
    "                    ]\n",
    "            s_lexemes = [stemmer.stem(word) for word in s_words]\n",
    "            words.append(s_words)\n",
    "            lexemes.append(s_lexemes)\n",
    "    return sentences, words, lexemes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences, words, lexemes = load_database(\"../../code/datasets/nlp/the old man and the sea.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_stems = set()\n",
    "for sentence in lexemes:\n",
    "    for stem in sentence:\n",
    "        unique_stems.add(stem)\n",
    "        \n",
    "index = 0\n",
    "indexed_stems = {}\n",
    "for stem in unique_stems:\n",
    "    indexed_stems[stem] = index\n",
    "    index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "##### Get the TF\n",
    "vectors_sentences = [[0] * len(indexed_stems) for i in range(len(lexemes))]\n",
    "index = 0\n",
    "for sentence in lexemes:\n",
    "    for stem in sentence:\n",
    "        vectors_sentences[index][indexed_stems[stem]] += 1\n",
    "    index += 1\n",
    "\n",
    "tf = [[0] * len(indexed_stems) for i in range(len(lexemes))]\n",
    "index = 0\n",
    "for sentence in lexemes:\n",
    "    for stem in sentence:\n",
    "        tf[index][indexed_stems[stem]] = vectors_sentences[index][indexed_stems[stem]] / len(sentence)\n",
    "    index += 1\n",
    "\n",
    "## Calculate the total amount of occurence for each word\n",
    "total_vector = [0] * len(indexed_stems)\n",
    "for vector in vectors_sentences:\n",
    "    index = 0\n",
    "    for element in vector:\n",
    "        total_vector[index] += element\n",
    "        index += 1\n",
    "        \n",
    "total_occs_per_sentence = [0] * len(indexed_stems)\n",
    "for indexed_stem in indexed_stems.items():\n",
    "    occs = 0\n",
    "    for vector in vectors_sentences:\n",
    "        if vector[indexed_stem[1]] > 0:\n",
    "            occs += 1\n",
    "    \n",
    "    total_occs_per_sentence[indexed_stem[1]] = occs\n",
    "\n",
    "##### Calculate IDF\n",
    "idf = [[0] * len(indexed_stems) for i in range(len(lexemes))]\n",
    "index = 0\n",
    "for sentence in lexemes:\n",
    "    for stem in sentence:\n",
    "        idf[index][indexed_stems[stem]] = math.log(len(lexemes) / total_occs_per_sentence[indexed_stems[stem]])\n",
    "    index += 1\n",
    "        \n",
    "\n",
    "tf_idf = [[0] * len(indexed_stems) for i in range(len(lexemes))]\n",
    "index = 0\n",
    "for sentence in lexemes:\n",
    "    for stem in sentence:\n",
    "        tf_idf[index][indexed_stems[stem]] = tf[index][indexed_stems[stem]] * idf[index][indexed_stems[stem]]\n",
    "    index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import spatial\n",
    "\n",
    "def amount_contains_sentence(bag, sentence):\n",
    "    amount = 0\n",
    "    for word in bag:\n",
    "        if word in sentence:\n",
    "            amount += 1\n",
    "            \n",
    "    return amount\n",
    "# TODO: write the code that will find ALL sentences which contain all words of query\n",
    "def exact_match(query):\n",
    "    result = []\n",
    "    stemmed_bag_query = [stemmer.stem(word.lower()) for word in query.split()]\n",
    "    \n",
    "    index = 0\n",
    "    for sentence in lexemes:\n",
    "        if amount_contains_sentence(stemmed_bag_query, sentence) == len(stemmed_bag_query):\n",
    "            print(index)\n",
    "            result.append(sentences[index])\n",
    "        index += 1\n",
    "    \n",
    "    return result\n",
    "\n",
    "# TODO: write the code that will find TOP sentences with THE BEST matches with query\n",
    "def ranked_match(query, top=5):\n",
    "    ranked_result = []\n",
    "    stemmed_bag_query = [0] * len(indexed_stems)\n",
    "    for word in query.split():\n",
    "        stem = stemmer.stem(word.lower())\n",
    "        stemmed_bag_query[indexed_stems[stem]] += 1 / total_vector[indexed_stems[stem]] \n",
    "    \n",
    "    for index in range(len(lexemes)):\n",
    "        simi = 1 - spatial.distance.cosine(stemmed_bag_query, tf_idf[index])\n",
    "        ranked_result.append((simi, sentences[index]))\n",
    "    \n",
    "    ranked_result.sort(reverse=True)\n",
    "    return [item for item in ranked_result[:top]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact match: \n",
      "709\n",
      "1403\n",
      "it was the yellow gulf weed that had made so much phosphorescence in the night.\n",
      "so he hooked a patch of yellow gulf weed with the gaff as they passed and shook it so that the small shrimps that were in it fell onto the planking of the skiff.\n",
      "\n",
      "Ranked match: \n",
      "(0.656776285338411, 'it was the yellow gulf weed that had made so much phosphorescence in the night.')\n",
      "(0.4074825976942933, 'so he hooked a patch of yellow gulf weed with the gaff as they passed and shook it so that the small shrimps that were in it fell onto the planking of the skiff.')\n",
      "(0.29698873034935236, 'there was yellow weed on the line but the old man knew that only made an added drag and he was pleased.')\n",
      "(0.2915515596793602, 'the dark water of the true gulf is the greatest healer that there is.')\n",
      "(0.23366898929817892, 'he saw the phosphorescence of the gulf weed in the water as he rowed over the part of the ocean that the fishermen called the great well because there was a sudden deep of seven hundred fathoms where all sorts of fish congregated because of the swirl the current made against the steep walls of the floor of the ocean.')\n"
     ]
    }
   ],
   "source": [
    "print(\"Exact match: \")\n",
    "print(*exact_match(\"yellow Gulf weeded\"), sep='\\n')\n",
    "print()\n",
    "print(\"Ranked match: \")\n",
    "print(*ranked_match(\"yellow Gulf weeded\"), sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
