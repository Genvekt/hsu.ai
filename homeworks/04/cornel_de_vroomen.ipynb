{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "\n",
    "sentences = []\n",
    "lines = open('../../code/datasets/nlp/the old man and the sea.txt').readlines()\n",
    "\n",
    "tokenized_sentences = []\n",
    "for line in lines:\n",
    "    line = line.strip().strip('\"').strip('`').strip(\"'\")\n",
    "    for sentence in re.split('\\.|\\?|!', line):\n",
    "        tokenized_sentences.append(nltk.word_tokenize(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', \"'ll\", 'be', 'back', 'when', 'I', 'have', 'the', 'sardines']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('I', 'PRP'),\n",
       " (\"'ll\", 'MD'),\n",
       " ('be', 'VB'),\n",
       " ('back', 'RB'),\n",
       " ('when', 'WRB'),\n",
       " ('I', 'PRP'),\n",
       " ('have', 'VBP'),\n",
       " ('the', 'DT'),\n",
       " ('sardines', 'NNS')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample sentences\n",
    "print(tokenized_sentences[201])\n",
    "nltk.pos_tag(tokenized_sentences[201])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('his', 'hope'), ('hope', 'and'), ('and', 'his'), ('his', 'confidence'), ('confidence', 'had'), ('had', 'never'), ('never', 'gone')]\n"
     ]
    }
   ],
   "source": [
    "def get_bigrams(sentence):\n",
    "    bigram = []\n",
    "    for i in range(len(sentence) - 1):\n",
    "        bigram.append((sentence[i].lower(), sentence[i + 1].lower()))\n",
    "    return bigram\n",
    "\n",
    "# sample\n",
    "print(get_bigrams(tokenized_sentences[101]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrammed_sentences = [get_bigrams(sentence) for sentence in tokenized_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 words: \n",
      "[('the', 2316), ('and', 1259), ('he', 1166), (',', 784), ('of', 540), ('i', 508), ('it', 494), ('to', 454), ('his', 446), ('was', 435), ('a', 397), ('in', 359), ('that', 299), ('fish', 285), ('man', 264), ('old', 248), ('but', 233), ('him', 230), ('not', 217), ('with', 205)]\n",
      "\n",
      "Top 20 bigrams: \n",
      "[('of the', 248), ('old man', 229), ('the old', 205), ('in the', 197), (\", ''\", 179), ('and the', 172), (', he', 161), ('and he', 151), ('he thought', 145), ('the fish', 139), ('he was', 107), ('on the', 100), ('the boy', 96), ('he said', 96), (\"'' the\", 89), ('it was', 88), (\"'' he\", 87), ('the line', 83), ('he had', 80), ('the water', 73)]\n",
      "\n",
      "Top 20 words via another way:\n",
      "[('the', 2316), ('and', 1259), ('he', 1166), (',', 784), ('of', 540), ('i', 508), ('it', 494), ('to', 454), ('his', 446), ('was', 435), ('a', 397), ('in', 359), ('that', 299), ('fish', 285), ('man', 264), ('old', 248), ('but', 233), ('him', 230), ('not', 217), ('with', 205)]\n",
      "\n",
      "Most used nouns: \n",
      "[('man NN', 262), ('fish NN', 215), ('line NN', 138), ('water NN', 105), ('boy NN', 99), ('hand NN', 90), ('head NN', 61), ('skiff NN', 46), ('shark NN', 46), ('time NN', 46), ('sun NN', 44), ('boat NN', 39), ('sea NN', 38), ('back NN', 34), ('bow NN', 31), ('side NN', 30), ('night NN', 30), ('nothing NN', 27), ('bird NN', 26), ('day NN', 25)]\n",
      "\n",
      "Most used actions: \n",
      "[('was VBD', 435), ('had VBD', 201), ('said VBD', 189), ('thought VBD', 161), ('were VBD', 138), ('did VBD', 61), ('saw VBD', 45), ('knew VBD', 43), ('came VBD', 41), ('felt VBD', 39), ('looked VBD', 36), ('went VBD', 35), ('made VBD', 32), ('took VBD', 30), ('held VBD', 24), ('watched VBD', 24), ('started VBD', 20), ('put VBD', 19), ('hit VBD', 17), ('asked VBD', 16)]\n"
     ]
    }
   ],
   "source": [
    "### Most common words\n",
    "word_map = {}\n",
    "for sentence in tokenized_sentences:\n",
    "    for word in sentence:\n",
    "        if word.lower() in word_map:\n",
    "            word_map[word.lower()] += 1\n",
    "        else:\n",
    "            word_map[word.lower()] = 1\n",
    "word_map = sorted(word_map.items(), key=lambda kv: kv[1], reverse=True)\n",
    "\n",
    "top_20 = []\n",
    "for i in range(20):\n",
    "    top_20.append(word_map[i])\n",
    "\n",
    "print(\"Top 20 words: \")\n",
    "print(top_20)\n",
    "print()\n",
    "\n",
    "\n",
    "### Most common bigrams\n",
    "bigram_map = {}\n",
    "for sentence in bigrammed_sentences:\n",
    "    for bigram in sentence:\n",
    "        key = bigram[0] + \" \" + bigram[1]\n",
    "        if key in bigram_map:\n",
    "            bigram_map[key] += 1\n",
    "        else:\n",
    "            bigram_map[key] = 1\n",
    "bigram_map = sorted(bigram_map.items(), key=lambda kv: kv[1], reverse=True)\n",
    "\n",
    "top_20 = []\n",
    "for i in range(20):\n",
    "    top_20.append(bigram_map[i])\n",
    "\n",
    "print(\"Top 20 bigrams: \")\n",
    "print(top_20)\n",
    "print()\n",
    "\n",
    "\n",
    "################ It can be done differently with NLTK: ################\n",
    "all_words = []\n",
    "for sentence in tokenized_sentences:\n",
    "    for word in sentence:\n",
    "        all_words.append(word)\n",
    "fdist = nltk.FreqDist(word.lower() for word in all_words)\n",
    "top_20 = fdist.most_common(20)\n",
    "\n",
    "print(\"Top 20 words via another way:\")\n",
    "print(top_20)\n",
    "print()\n",
    "\n",
    "\n",
    "############ ADVANCED: most popular nouns and verbs ############\n",
    "all_tagged_words = []\n",
    "\n",
    "for sentence in tokenized_sentences:\n",
    "    all_tagged_words += nltk.pos_tag(sentence)\n",
    "\n",
    "word_map = {}\n",
    "for tokenized_word in all_tagged_words:\n",
    "    key = tokenized_word[0] + \" \" + tokenized_word[1]\n",
    "    if key in word_map:\n",
    "        word_map[key] += 1\n",
    "    else:\n",
    "        word_map[key] = 1\n",
    "word_map = sorted(word_map.items(), key=lambda kv: kv[1], reverse=True)\n",
    "\n",
    "top_20_nouns = []\n",
    "top_20_actions = []\n",
    "\n",
    "index = 0\n",
    "while len(top_20_nouns) < 20 and index < len(word_map):\n",
    "    if word_map[index][0].split()[1] == \"NN\":\n",
    "        top_20_nouns.append(word_map[index])\n",
    "    index += 1\n",
    "\n",
    "index = 0\n",
    "while len(top_20_actions) < 20 and index < len(word_map):\n",
    "    if word_map[index][0].split()[1] == \"VBD\":\n",
    "        top_20_actions.append(word_map[index])\n",
    "    index += 1\n",
    "\n",
    "print(\"Most used nouns: \")\n",
    "print(top_20_nouns)\n",
    "print()\n",
    "print(\"Most used actions: \")\n",
    "print(top_20_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
